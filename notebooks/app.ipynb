{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as pilImage\n",
    "from fastai.vision.all import *\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "\n",
    "from src.utils import *\n",
    "from src.gradcam import GuidedGradCam\n",
    "from src.augmentationImpactAnalyzer import AugmentationImpactAnalyzer \n",
    "\n",
    "ROOT_DIR =  Path('../')\n",
    "DATA_PATH = ROOT_DIR/'data/'\n",
    "IMGS_PATH = ROOT_DIR/'imgs/'\n",
    "MODEL_API_PATH = ROOT_DIR/'model_api'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load IMAGENETTE_160 and \n",
    "#path = untar_data(URLs.IMAGENETTE_160,dest=DATA_PATH)\n",
    "\n",
    "# load the classes to get the right labels\n",
    "#import urllib.request, json \n",
    "#with urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json') as url:\n",
    "#    classes = json.loads(url.read().decode())\n",
    "#f_to_idx = {val[0]:idx for idx,val in classes.items()}\n",
    "#my_classes = [f_to_idx[f.name] for f in (path/\"train\").ls()]\n",
    "\n",
    "#def save_pickle(item, path):\n",
    "#    with open(path, 'wb') as handle:\n",
    "#        pickle.dump(item, handle)\n",
    "#\n",
    "#save_pickle(my_classes, MODEL_API_PATH/'IMAGENETTE_160_classes')\n",
    "#save_pickle(classes, MODEL_API_PATH/'IMAGENETTE_classes')\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        item = pickle.load(handle)\n",
    "    return item\n",
    "\n",
    "classes = load_pickle(MODEL_API_PATH/'IMAGENETTE_classes')\n",
    "my_classes = load_pickle(MODEL_API_PATH/'IMAGENETTE_160_classes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ap/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register hooks for:\n",
      "layer4.2\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "ggc = GuidedGradCam(model,use_cuda,target_type='classification', layer_ids=['layer4.2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# use imagenet stats        \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "size = 160\n",
    "img = pilImage.open(IMGS_PATH/'example_input.png').resize((size,size))\n",
    "aia = AugmentationImpactAnalyzer(img,\n",
    "                                model,\n",
    "                                cuda=use_cuda,\n",
    "                                add_output_act=True,\n",
    "                                restrict_classes={i:classes[i][1] for i in my_classes},\n",
    "                                normalize=normalize,\n",
    "                                guided_grad_cam=ggc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- test image transformation class ---------- \n",
    "#aia.tfms(crop_size=140,highlight_act='gradcam',perspective_w=180,perspective_h=180,perspective_d=20,erase_w=10,erase_h=10,rotate_ang=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = aia.img.shape\n",
    "\n",
    "import ipywidgets as widgets\n",
    "def create_gif_on_click(change):\n",
    "    os.makedirs('../imgs/',exist_ok=True)\n",
    "    aia.create_gif(IMGS_PATH/'results.gif')\n",
    "\n",
    "btn_create_gif = widgets.Button(description='Create gif')\n",
    "btn_create_gif.on_click(create_gif_on_click)\n",
    "\n",
    "btn_act_loc = widgets.RadioButtons(\n",
    "    options=['none','gradcam','guided-gradient','guided-gradcam',],\n",
    "    description='Activation Localization:',\n",
    "    disabled=False)\n",
    "\n",
    "sl_brightness = widgets.FloatSlider(value=1,min=0.1,max=4,step=0.2)\n",
    "box_brightness = widgets.VBox([widgets.HTML('<em>Brightness</em>'),sl_brightness])\n",
    "\n",
    "sl_crop_size = widgets.IntSlider(value=width,min=33,max=width,step=5)\n",
    "box_crop = widgets.VBox([widgets.HTML('<em>Center Crop</em>'),sl_crop_size])\n",
    "\n",
    "sl_rotate = widgets.IntSlider(value=0,min=0,max=360,step=5)\n",
    "box_rotate = widgets.VBox([widgets.HTML('<em>Rotation Angle</em>'),sl_rotate])\n",
    "sl_perspective_w = widgets.IntSlider(value=width,min=0,max=width,step=5)\n",
    "sl_perspective_h = widgets.IntSlider(value=height,min=0,max=height,step=5)\n",
    "sl_perspective_d = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "box_perspective = widgets.VBox([widgets.HTML('<em>Perspective Distortion</em>'),sl_perspective_w,sl_perspective_h,sl_perspective_d])\n",
    "\n",
    "sl_erase_i = widgets.IntSlider(value=0,min=0,max=width,step=5)\n",
    "sl_erase_j = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "sl_erase_w = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "sl_erase_h = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "box_erase = widgets.VBox([widgets.HTML('<em>Erase Box</em>'),sl_erase_i,sl_erase_j,sl_erase_w, sl_erase_h])\n",
    "\n",
    "btn_upload = widgets.FileUpload(description='Your Image')\n",
    "\n",
    "tfm_args_sl = { \"brightness\":sl_brightness,\n",
    "                \"crop_size\":sl_crop_size,\n",
    "                \"activation_localization\":btn_act_loc, \n",
    "                \"rotate_ang\":sl_rotate,\n",
    "                \"perspective_w\":sl_perspective_w,\n",
    "                \"perspective_h\":sl_perspective_h,\n",
    "                \"perspective_d\":sl_perspective_d,\n",
    "                \"erase_i\":sl_erase_i,\n",
    "                \"erase_j\":sl_erase_j,\n",
    "                \"erase_w\":sl_erase_w,\n",
    "                \"erase_h\":sl_erase_h}\n",
    "#tfm_args = {k:v.value for k,v in tfm_args_sl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdc51da42cfc4e44b911e1bfbe7d8d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FileUpload(value={}, description='Your Image'), VBox(children=(HTML(value='<em>Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#aia.reset(PILImage.create((path/\"train/n02102040\").ls()[32]).resize((size,size)))  \n",
    "aia.reset(img)\n",
    "out_tfms = widgets.interactive_output(aia.tfms, tfm_args_sl)\n",
    "gui = widgets.HBox([widgets.VBox([btn_upload, box_brightness, box_crop, box_rotate, box_perspective, box_erase,\n",
    "                            btn_act_loc, btn_create_gif]), out_tfms])\n",
    "\n",
    "def on_upload_change(change):\n",
    "    aia.reset(PILImage.create(btn_upload.data[-1]))\n",
    "    if btn_act_loc.value != 'none':\n",
    "        btn_act_loc.value = 'none'\n",
    "    else:\n",
    "        # quick (ugly) hack to reload the image for sure\n",
    "        if sl_crop_size.value != width:\n",
    "            sl_crop_size.value = width\n",
    "        else:\n",
    "            sl_crop_size.value = sl_crop_size.value - sl_crop_size.step \n",
    "    #aia.tfms(**tfm_args) #doesn't reset the gui output\n",
    "\n",
    "btn_upload.observe(on_upload_change, names='_counter')\n",
    "display(gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open Gif](../imgs/results.gif \"segment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
