{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from fastai.vision.all import *\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as pilImage\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.utils import *\n",
    "from src.gradcam import GuidedGradCam\n",
    "from src.augmentationImpactAnalyzer import AugmentationImpactAnalyzer \n",
    "\n",
    "ROOT_DIR =  Path('../')\n",
    "DATA_PATH = ROOT_DIR/'data/'\n",
    "IMGS_PATH = ROOT_DIR/'imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IMAGENETTE_160 and \n",
    "path = untar_data(URLs.IMAGENETTE_160,dest=DATA_PATH)\n",
    "#print((path/'train').ls())\n",
    "\n",
    "## display an example\n",
    "fname = (path/\"train/n02102040\").ls()[1]\n",
    "#PILImage.create(fname).resize((160,160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classes to get the right labels\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json') as url:\n",
    "    classes = json.loads(url.read().decode())\n",
    "f_to_idx = {val[0]:idx for idx,val in classes.items()}\n",
    "my_classes = [f_to_idx[f.name] for f in (path/\"train\").ls()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ap/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "register hooks for:\n",
      "layer4.2\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "ggc = GuidedGradCam(model,use_cuda,target_type='classification', layer_ids=['layer4.2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- test gradcam ---------- \n",
    "#fname = (path/\"train/n02102040\").ls()[1]\n",
    "#img = PILImage.create(fname)#.resize((160,160))\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                 std=[0.229, 0.224, 0.225])\n",
    "#x = normalize(transforms.ToTensor()(img)).unsqueeze(0)\n",
    "#heatmap, gb, cam_gb = ggc(x.requires_grad_(True))\n",
    "#\n",
    "#fig, ax = plt.subplots(1,4,figsize=(18,6))\n",
    "#ax[0].imshow(img)\n",
    "#cam = np.float32(img)+np.float32(heatmap)\n",
    "#ax[1].imshow(arr_to_img(cam))\n",
    "#ax[2].imshow(gb)\n",
    "#ax[3].imshow(cam_gb)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# use imagenet stats        \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "size = 160\n",
    "img = PILImage.create(fname).resize((size,size))\n",
    "aia = AugmentationImpactAnalyzer(img,\n",
    "                                model,\n",
    "                                cuda=use_cuda,\n",
    "                                add_output_act=True,\n",
    "                                restrict_classes={i:classes[i][1] for i in my_classes},\n",
    "                                normalize=normalize,\n",
    "                                guided_grad_cam=ggc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- test image transformation class ---------- \n",
    "#aia.tfms(crop_size=140,highlight_act='gradcam',perspective_w=180,perspective_h=180,perspective_d=20,erase_w=10,erase_h=10,rotate_ang=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = aia.img.shape\n",
    "\n",
    "import ipywidgets as widgets\n",
    "def create_gif_on_click(change):\n",
    "    os.makedirs('../imgs/',exist_ok=True)\n",
    "    aia.create_gif(IMGS_PATH/'results.gif')\n",
    "\n",
    "btn_create_gif = widgets.Button(description='Create gif')\n",
    "btn_create_gif.on_click(create_gif_on_click)\n",
    "\n",
    "btn_act_loc = widgets.RadioButtons(\n",
    "    options=['none','gradcam','guided-gradient','guided-gradcam',],\n",
    "    description='Activation Localization:',\n",
    "    disabled=False)\n",
    "\n",
    "sl_brightness = widgets.FloatSlider(value=1,min=0.1,max=4,step=0.2)\n",
    "box_brightness = widgets.VBox([widgets.HTML('<em>Brightness</em>'),sl_brightness])\n",
    "\n",
    "sl_crop_size = widgets.IntSlider(value=width,min=33,max=width,step=5)\n",
    "box_crop = widgets.VBox([widgets.HTML('<em>Center Crop</em>'),sl_crop_size])\n",
    "\n",
    "sl_rotate = widgets.IntSlider(value=0,min=0,max=360,step=5)\n",
    "box_rotate = widgets.VBox([widgets.HTML('<em>Rotation Angle</em>'),sl_rotate])\n",
    "sl_perspective_w = widgets.IntSlider(value=width,min=0,max=width,step=5)\n",
    "sl_perspective_h = widgets.IntSlider(value=height,min=0,max=height,step=5)\n",
    "sl_perspective_d = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "box_perspective = widgets.VBox([widgets.HTML('<em>Perspective Distortion</em>'),sl_perspective_w,sl_perspective_h,sl_perspective_d])\n",
    "\n",
    "sl_erase_i = widgets.IntSlider(value=0,min=0,max=width,step=5)\n",
    "sl_erase_j = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "sl_erase_w = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "sl_erase_h = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "box_erase = widgets.VBox([widgets.HTML('<em>Erase Box</em>'),sl_erase_i,sl_erase_j,sl_erase_w, sl_erase_h])\n",
    "\n",
    "btn_upload = widgets.FileUpload(description='Your Image')\n",
    "\n",
    "tfm_args_sl = { \"brightness\":sl_brightness,\n",
    "                \"crop_size\":sl_crop_size,\n",
    "                \"activation_localization\":btn_act_loc, \n",
    "                \"rotate_ang\":sl_rotate,\n",
    "                \"perspective_w\":sl_perspective_w,\n",
    "                \"perspective_h\":sl_perspective_h,\n",
    "                \"perspective_d\":sl_perspective_d,\n",
    "                \"erase_i\":sl_erase_i,\n",
    "                \"erase_j\":sl_erase_j,\n",
    "                \"erase_w\":sl_erase_w,\n",
    "                \"erase_h\":sl_erase_h}\n",
    "#tfm_args = {k:v.value for k,v in tfm_args_sl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "362f9ad822ce48bca803402f279d207a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FileUpload(value={}, description='Your Image'), VBox(children=(HTML(value='<em>Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#aia.reset(PILImage.create((path/\"train/n02102040\").ls()[32]).resize((size,size)))  \n",
    "aia.reset(img)\n",
    "out_tfms = widgets.interactive_output(aia.tfms, tfm_args_sl)\n",
    "gui = widgets.HBox([widgets.VBox([btn_upload, box_brightness, box_crop, box_rotate, box_perspective, box_erase,\n",
    "                            btn_act_loc, btn_create_gif]), out_tfms])\n",
    "\n",
    "def on_upload_change(change):\n",
    "    aia.reset(PILImage.create(btn_upload.data[-1]))\n",
    "    if btn_act_loc.value != 'none':\n",
    "        btn_act_loc.value = 'none'\n",
    "    else:\n",
    "        # quick (ugly) hack to reload the image for sure\n",
    "        if sl_crop_size.value != width:\n",
    "            sl_crop_size.value = width\n",
    "        else:\n",
    "            sl_crop_size.value = sl_crop_size.value - sl_crop_size.step \n",
    "    #aia.tfms(**tfm_args) #doesn't reset the gui output\n",
    "\n",
    "btn_upload.observe(on_upload_change, names='_counter')\n",
    "display(gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open Gif](../imgs/results.gif \"segment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the images separate \n",
    "#os.makedirs('../imgs/img_list/',exist_ok=True)\n",
    "#os.makedirs('../imgs/results/',exist_ok=True)\n",
    "#\n",
    "#for i,pic in enumerate(imgs):\n",
    "#    pic.save(fp=f'../imgs/img_list/img_{str(i).zfill(2)}.png', format='PNG')\n",
    "\n",
    "# to use ffmpeg I need to deactivate conda and call this in the terminal\n",
    "# but it doesn't seem for me that the results are better, so I leave it for now\n",
    "#ffmpeg -f image2 -i imgs/img_list/img_%02d.png -vf scale=2480:-1:sws_dither=ed,palettegen imgs/results/palette.png -y\n",
    "#ffmpeg -f image2 -framerate 10. -i imgs/img_list/img_%02d.png imgs/results/img.flv -y\n",
    "#ffmpeg -i imgs/results/img.flv -i imgs/results/palette.png -filter_complex \"fps=10,scale=248:-1:flags=lanczos[x];[x][1:v]paletteuse\" imgs/results/test.gif -y\n",
    "#ffmpeg -i imgs/results/img.flv -i imgs/results/palette.png -filter_complex \"fps=10,scale=248:-1:flags=lanczos[x];[x][1:v]paletteuse\" -loop -1 imgs/results/test_no_loop.gif -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
