{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from fastai.vision.all import *\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as pilImage\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\") \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.utils import *\n",
    "from src.gradcam import *\n",
    "\n",
    "ROOT_DIR =  Path('../')\n",
    "DATA_PATH = ROOT_DIR/'data/'\n",
    "IMGS_PATH = ROOT_DIR/'imgs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load IMAGENETTE_160 and \n",
    "path = untar_data(URLs.IMAGENETTE_160,dest=DATA_PATH)\n",
    "#print((path/'train').ls())\n",
    "\n",
    "## display an example\n",
    "fname = (path/\"train/n02102040\").ls()[1]\n",
    "#PILImage.create(fname).resize((160,160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the classes to get the right labels\n",
    "import urllib.request, json \n",
    "with urllib.request.urlopen('https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json') as url:\n",
    "    classes = json.loads(url.read().decode())\n",
    "f_to_idx = {val[0]:idx for idx,val in classes.items()}\n",
    "my_classes = [f_to_idx[f.name] for f in (path/\"train\").ls()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/ap/.cache/torch/hub/pytorch_vision_v0.6.0\n",
      "Using cache found in /Users/ap/.cache/torch/hub/pytorch_vision_v0.6.0\n"
     ]
    }
   ],
   "source": [
    "# modify this function to use your custom model\n",
    "def load_my_model():\n",
    "    return torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "\n",
    "device = 'cpu'\n",
    "use_cuda = True if device == 'cuda' else False\n",
    "\n",
    "model = load_my_model()\n",
    "#eval() is very important, otherwise the results are not useable \n",
    "#(I don't understand why train mode computes almost always the same results, no matter what's the input img)\n",
    "model = model.eval() \n",
    "\n",
    "# I need to pass 2 separate versions of the model because the relu function gets overwritten (here: recursive_relu_apply(self.model) )\n",
    "# todo: make it possible to use only original model version (above). Note that it seems quite tricky to because I need different gradients for cam and guided backpropagation.\n",
    "grad_cam = model#load_my_model()\n",
    "gb_model = load_my_model()\n",
    "\n",
    "grad_cam = GradCam(model=grad_cam, feature_module=grad_cam.layer4, \\\n",
    "                   target_layer_names=[\"2\"], use_cuda=use_cuda)\n",
    "gb_model = GuidedBackpropReLUModel(model=gb_model, use_cuda=use_cuda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation localization functions\n",
    "# Note: this functions work with global grad_cam, gb_model, therefore I define them here (could pass the models also as argument)\n",
    "# Because the AugmentationImpactAnalyzer below works with this functions I also leave it here in the notebook instead of putting it into src.\n",
    "def get_heatmap(x, target_index = None):\n",
    "    mask = grad_cam(x, target_index)\n",
    "    heatmap = arr_to_img(mask,cmap='inferno')\n",
    "    return heatmap\n",
    "\n",
    "def get_gradient_act(x, target_index = None):\n",
    "    _ = grad_cam(x, target_index) # todo: find out how to get rid of this unnecessary call (if I don't call it the gradients are computed differently, why???)\n",
    "    gb = gb_model(x, index=target_index)\n",
    "    return deprocess_image(gb.transpose((1, 2, 0)))\n",
    "\n",
    "def find_relevant_input(x, target_index = None):\n",
    "    '''Function to compute grad-cam, returns also cam heatmap and plain backwards-gradient\n",
    "       If target_index is None, returns the map for the highest scoring category.\n",
    "       Otherwise, targets the requested index.'''\n",
    "    mask = grad_cam(x, target_index)\n",
    "    heatmap = arr_to_img(mask,cmap='inferno')\n",
    "\n",
    "    gb = gb_model(x, index=target_index)\n",
    "    gb = gb.transpose((1, 2, 0))\n",
    "    \n",
    "    cam_mask = np.stack([mask, mask, mask],2)\n",
    "    cam_gb = deprocess_image(cam_mask*gb)\n",
    "    gb = deprocess_image(gb)\n",
    "    return heatmap, gb, cam_gb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- test gradcam ---------- \n",
    "#size= 160\n",
    "#img = PILImage.create(fname).resize((size,size))\n",
    "#normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                 std=[0.229, 0.224, 0.225])\n",
    "#x = normalize(transforms.ToTensor()(img)).unsqueeze(0)\n",
    "#heatmap, gb, cam_gb = find_relevant_input(x.requires_grad_(True))\n",
    "#\n",
    "#fig, ax = plt.subplots(1,4,figsize=(18,6))\n",
    "#ax[0].imshow(img)\n",
    "#cam = np.float32(img)+np.float32(heatmap)\n",
    "#ax[1].imshow(arr_to_img(cam))\n",
    "#ax[2].imshow(gb)\n",
    "#ax[3].imshow(cam_gb)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class AugmentationImpactAnalyzer():\n",
    "    def __init__(self,img,model=None,cuda=False,add_output_act=False,restrict_classes=None,normalize=None):\n",
    "        self.img_orig = img.copy()\n",
    "        self.img = img.copy()\n",
    "        self.out_img = img.copy()\n",
    "        self.x_orig = transforms.ToTensor()(img).unsqueeze(0)\n",
    "        self.x_orig = self.x_orig.cuda() if cuda else self.x_orig\n",
    "        self.model = model.cuda() if cuda else model\n",
    "        self.add_output_act = add_output_act\n",
    "        self.images = []\n",
    "        self.activations = []\n",
    "        self.restrict_classes = restrict_classes\n",
    "        self.class_idx = [int(i) for i in restrict_classes.keys()] if restrict_classes is not None else None\n",
    "        self.normalize = normalize if normalize is not None else lambda x: x\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def reset(self,new_img=None):\n",
    "        self.img_orig = new_img if new_img is not None else self.img_orig\n",
    "        self.img = self.img_orig.copy()\n",
    "        self.x_orig = transforms.ToTensor()(new_img).unsqueeze(0) if new_img is not None else self.x_orig \n",
    "        self.x_orig = self.x_orig.cuda() if self.cuda else self.x_orig\n",
    "        self.images = []\n",
    "        self.activations = []\n",
    "        \n",
    "    def tfm_brightness(self, brightness):\n",
    "        x =  TF.adjust_brightness(self.x, brightness)\n",
    "        return x\n",
    "    \n",
    "    def tfm_centercrop(self, crop_size):\n",
    "        x =  transforms.CenterCrop(crop_size)(self.x)\n",
    "        return x  \n",
    "    \n",
    "    def tfm_perspective(self, perspective_w=None, perspective_h=None, perspective_d=0):\n",
    "        width, height = self.x.shape[2:]\n",
    "        w = perspective_w if perspective_w is not None else width\n",
    "        h = perspective_h if perspective_h is not None else height\n",
    "        d = perspective_d\n",
    "        startpoints = [[0, 0], [width - 1, 0], [width - 1, height - 1], [0, height - 1]]\n",
    "        endpoints = [[0, 0], [w, d], [w, h], [0, h]] \n",
    "        return TF.perspective(self.x, startpoints, endpoints, pilImage.BILINEAR)\n",
    "    \n",
    "    def tfm_erase(self, erase_i=0, erase_j=0, erase_h=0, erase_w=0):\n",
    "        return TF.erase(self.x,erase_i,erase_j,erase_h,erase_w,0)\n",
    "    \n",
    "    def tfm_rotate(self, angle):\n",
    "        return TF.rotate(self.x,angle=angle)\n",
    "    \n",
    "    def tfms(self, activation_localization=None, show=True, brightness=None, crop_size=None, rotate_ang=None,  **args):\n",
    "        '''Run function to perform several transformations. \n",
    "           Args: \n",
    "               activation_localization (str): {'none','cam','gradient','gradcam'}, default None\n",
    "                                              Mode for visualizing the parts of the image that affected the model output most\n",
    "           todo: add more docu\n",
    "        '''\n",
    "        self.x = self.x_orig\n",
    "        # first run all augmentation methods\n",
    "        # adjust brightness\n",
    "        if brightness is not None:\n",
    "            self.x = self.tfm_brightness(brightness)\n",
    "        # center crop the image\n",
    "        if crop_size is not None:\n",
    "            self.x = self.tfm_centercrop(crop_size)\n",
    "        # rotate the image\n",
    "        if rotate_ang is not None:\n",
    "            self.x = self.tfm_rotate(rotate_ang)\n",
    "        # perspective distorition    \n",
    "        perspective_args = {k:v for k,v in args.items() if 'perspective' in k}\n",
    "        if len(perspective_args):\n",
    "            self.x = self.tfm_perspective(**perspective_args)\n",
    "        # erase part of the image    \n",
    "        erase_args = {k:v for k,v in args.items() if 'erase' in k}\n",
    "        if len(erase_args):\n",
    "            self.x = self.tfm_erase(**erase_args)\n",
    "                \n",
    "        # then run activation localization\n",
    "        if (activation_localization is not None) & (activation_localization != 'none'):\n",
    "            x_input = self.normalize(self.x).requires_grad_(True)\n",
    "            if activation_localization == 'cam':\n",
    "                heatmap = get_heatmap(x_input)\n",
    "                # need to renormalize the image before adding it to another image\n",
    "                cam = tensor_to_np_img(min_max_scaler(self.x[0].detach())) + np.float32(heatmap)\n",
    "                self.out_img = arr_to_img(cam)\n",
    "            elif activation_localization == 'gradient':\n",
    "                self.out_img = get_gradient_act(x_input)\n",
    "            elif activation_localization == 'gradcam':\n",
    "                heatmap, gb, cam_gb = find_relevant_input(x_input)\n",
    "                self.out_img = cam_gb\n",
    "            else:\n",
    "                assert 0, f'activation_localization \"{activation_localization}\" not known'\n",
    "        else:\n",
    "            self.out_img = transforms.ToPILImage()(self.x[0])\n",
    "            \n",
    "        # then add model activation \n",
    "        # Note: Here we use a different model as in the activation localization. \n",
    "        # todo: use same model and use reuse model output from forward pass above (see also comments when loading the models)   \n",
    "        if self.add_output_act:\n",
    "            out = self.model(self.normalize(self.x)).cpu().detach().numpy()\n",
    "            if self.restrict_classes is not None:\n",
    "                out = out[:,self.class_idx]\n",
    "            self.activations.append(out)\n",
    "            self.out_img = self.combine_activation_with_img(self.out_img, out)\n",
    "        self.images.append(self.out_img)\n",
    "        \n",
    "        if show:\n",
    "            fig,ax = plt.subplots()\n",
    "            my_xticks = self.restrict_classes.values()\n",
    "            nr_classes = len(self.restrict_classes)\n",
    "            h,w,d = self.out_img.shape\n",
    "            step_size = w/nr_classes\n",
    "            ax.set_xticks(np.arange(nr_classes)*step_size+(step_size//2))\n",
    "            ax.set_xticklabels(my_xticks,rotation=90)\n",
    "            ax.imshow(self.out_img)\n",
    "    \n",
    "    def combine_activation_with_img(self, img, out):\n",
    "        # to make flatten activation better visible, the images will be streched by a factor of 20 \n",
    "        # and the width is adapted to the original image width\n",
    "        shape = (20,img.shape[1])\n",
    "\n",
    "        # combine transformed images and their layer activations\n",
    "        img_comb = np.vstack([img, arr_to_img(imresize(out,shape), 'inferno') ])\n",
    "        return img_comb\n",
    "    \n",
    "    def create_gif(self,path): \n",
    "        # to make flatten activation better visible, the images will be streched by a factor of 20 and the width is adapted to the original image width\n",
    "        w,h,d = self.images[0].shape\n",
    "        transform_act = lambda x: arr_to_img(imresize(x,(20,w)), 'inferno') \n",
    "\n",
    "        # combine sequence of transformed images and their layer activations\n",
    "        imgs_combs = [ np.vstack([arr_to_img(img).resize((w,h)), transform_act(act) ]) for img, act in zip(self.images,self.activations)  ]\n",
    "\n",
    "        # transform to images, add backward loop and store as gif\n",
    "        first_img, *imgs = [ pilImage.fromarray( img ) for img in imgs_combs]\n",
    "        imgs += [img for img in imgs[::-1]]\n",
    "        first_img.save(fp=path, format='GIF', append_images=imgs, save_all=True, duration=100, loop=0)\n",
    "\n",
    "# use imagenet stats        \n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "size = 160\n",
    "img = PILImage.create(fname).resize((size,size))\n",
    "aia = AugmentationImpactAnalyzer(img,\n",
    "                                model,\n",
    "                                cuda=use_cuda,\n",
    "                                add_output_act=True,\n",
    "                                restrict_classes={i:classes[i][1] for i in my_classes},\n",
    "                                normalize=normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ---------- test image transformation class ---------- \n",
    "#aia.tfms(crop_size=140,highlight_act='gradcam',perspective_w=180,perspective_h=180,perspective_d=20,erase_w=10,erase_h=10,rotate_ang=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = 160, 160 #aia.img.shape\n",
    "\n",
    "import ipywidgets as widgets\n",
    "def create_gif_on_click(change):\n",
    "    os.makedirs('../imgs/',exist_ok=True)\n",
    "    aia.create_gif(IMGS_PATH/'results.gif')\n",
    "\n",
    "btn_create_gif = widgets.Button(description='Create gif')\n",
    "btn_create_gif.on_click(create_gif_on_click)\n",
    "\n",
    "btn_act_loc = widgets.RadioButtons(\n",
    "    options=['none','cam','gradient','gradcam',],\n",
    "    description='Activation Localization:',\n",
    "    disabled=False)\n",
    "\n",
    "sl_brightness = widgets.FloatSlider(value=1,min=0.1,max=4,step=0.2)\n",
    "box_brightness = widgets.VBox([widgets.HTML('<em>Brightness</em>'),sl_brightness])\n",
    "\n",
    "sl_crop_size = widgets.IntSlider(value=width,min=33,max=width,step=5)\n",
    "box_crop = widgets.VBox([widgets.HTML('<em>Center Crop</em>'),sl_crop_size])\n",
    "\n",
    "sl_rotate = widgets.IntSlider(value=0,min=0,max=360,step=5)\n",
    "box_rotate = widgets.VBox([widgets.HTML('<em>Rotation Angle</em>'),sl_rotate])\n",
    "sl_perspective_w = widgets.IntSlider(value=width,min=0,max=width,step=5)\n",
    "sl_perspective_h = widgets.IntSlider(value=height,min=0,max=height,step=5)\n",
    "sl_perspective_d = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "box_perspective = widgets.VBox([widgets.HTML('<em>Perspective Distortion</em>'),sl_perspective_w,sl_perspective_h,sl_perspective_d])\n",
    "\n",
    "sl_erase_i = widgets.IntSlider(value=0,min=0,max=width,step=5)\n",
    "sl_erase_j = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "sl_erase_w = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "sl_erase_h = widgets.IntSlider(value=0,min=0,max=height,step=5)\n",
    "box_erase = widgets.VBox([widgets.HTML('<em>Erase Box</em>'),sl_erase_i,sl_erase_j,sl_erase_w, sl_erase_h])\n",
    "\n",
    "btn_upload = widgets.FileUpload(description='Your Image')\n",
    "\n",
    "tfm_args_sl = { \"brightness\":sl_brightness,\n",
    "                \"crop_size\":sl_crop_size,\n",
    "                \"activation_localization\":btn_act_loc, \n",
    "                \"rotate_ang\":sl_rotate,\n",
    "                \"perspective_w\":sl_perspective_w,\n",
    "                \"perspective_h\":sl_perspective_h,\n",
    "                \"perspective_d\":sl_perspective_d,\n",
    "                \"erase_i\":sl_erase_i,\n",
    "                \"erase_j\":sl_erase_j,\n",
    "                \"erase_w\":sl_erase_w,\n",
    "                \"erase_h\":sl_erase_h}\n",
    "#tfm_args = {k:v.value for k,v in tfm_args_sl.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ae7b82ab4f4df381d708557f96ebb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(FileUpload(value={}, description='Your Image'), VBox(children=(HTML(value='<em>Bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "aia.reset(PILImage.create((path/\"train/n02102040\").ls()[32]).resize((size,size)))  \n",
    "out_tfms = widgets.interactive_output(aia.tfms, tfm_args_sl)\n",
    "gui = widgets.HBox([widgets.VBox([btn_upload, box_brightness, box_crop, box_rotate, box_perspective, box_erase,\n",
    "                            btn_act_loc, btn_create_gif]), out_tfms])\n",
    "\n",
    "def on_upload_change(change):\n",
    "    aia.reset(PILImage.create(btn_upload.data[-1]))\n",
    "    if btn_act_loc.value != 'none':\n",
    "        btn_act_loc.value = 'none'\n",
    "    else:\n",
    "        # quick (ugly) hack to reload the image for sure\n",
    "        if sl_crop_size.value != width:\n",
    "            sl_crop_size.value = width\n",
    "        else:\n",
    "            sl_crop_size.value = sl_crop_size.value - sl_crop_size.step \n",
    "    #aia.tfms(**tfm_args) #doesn't reset the gui output\n",
    "\n",
    "btn_upload.observe(on_upload_change, names='_counter')\n",
    "display(gui)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Open Gif](../imgs/results.gif \"segment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save the images separate \n",
    "#os.makedirs('../imgs/img_list/',exist_ok=True)\n",
    "#os.makedirs('../imgs/results/',exist_ok=True)\n",
    "#\n",
    "#for i,pic in enumerate(imgs):\n",
    "#    pic.save(fp=f'../imgs/img_list/img_{str(i).zfill(2)}.png', format='PNG')\n",
    "\n",
    "# to use ffmpeg I need to deactivate conda and call this in the terminal\n",
    "# but it doesn't seem for me that the results are better, so I leave it for now\n",
    "#ffmpeg -f image2 -i imgs/img_list/img_%02d.png -vf scale=2480:-1:sws_dither=ed,palettegen imgs/results/palette.png -y\n",
    "#ffmpeg -f image2 -framerate 10. -i imgs/img_list/img_%02d.png imgs/results/img.flv -y\n",
    "#ffmpeg -i imgs/results/img.flv -i imgs/results/palette.png -filter_complex \"fps=10,scale=248:-1:flags=lanczos[x];[x][1:v]paletteuse\" imgs/results/test.gif -y\n",
    "#ffmpeg -i imgs/results/img.flv -i imgs/results/palette.png -filter_complex \"fps=10,scale=248:-1:flags=lanczos[x];[x][1:v]paletteuse\" -loop -1 imgs/results/test_no_loop.gif -y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
